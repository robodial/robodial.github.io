#### Part of [SIGDIAL 2018](http://www.sigdial.org/workshops/conference19/)

#### July 12-14, 2018

RMIT University

Melbourne, Australia
	
**Papers due March 11 for SIGDIAL (_Final pdf due March 18_) or April 28 for late-breaking and work-in-progress papers**

## Overview

Recent technologies have brought conversational robots out of the lab and into the homes and workplaces of real users. Dialogue is now actively taking place with robots and other smart devices to understand, operate, navigate, and manipulate physical space. _Physically situated dialogue_ distinguishes itself from other forms of dialogue in that it (1) takes place in a physical space, (2) refers to the shared surroundings of dialogue partners, and (3) involves a physical agent that can make actions in the world. There is a growing need for showcasing bi-directional dialogue work that draws on language grounding, models of vision and language, as well as dialogue that allows physically situated agents to ask for clarification and provide updates on their internal states.

### Call for Papers

Our objectives in this special session are to showcase recent and ongoing work on physically situated dialogue, and to identify paths forward in this space from research across communities including dialogue, robotics, computer vision, NLP, and AI. The special session will feature presentations, a poster session, and a panel discussion comprising a mix of experts in the topic area. We welcome submissions on any topic related to physically situated dialogue, including but not limited to:

 - Interaction studies with smart-home devices
 - Learning from demonstration through natural language dialogue
 - Explainable AI in physical spaces
 - Representations of physical surroundings / world modeling to support grounded communication
		<li>Embodied visual question answering and/or generation</li>
		<li>Empirical studies of human-robot dialogue (Wizard-of-Oz based, simulated, or semi-autonomous)</li>
		<li>Computational models of dialogue management and/or turn-taking with physical agents</li>
		<li>Methods of building or leveraging common ground with physical agents in real-world or simulated environments</li>
		<li>Corpora of physically situated dialogue (Wizard-of-Oz based or otherwise)</li>
		<li>Multimodal information processing to support dialogue (including speech, gaze, gesture)</li>
		<li>Physical embodiment, voice, or personification of robots and their effects on human-robot dialogue</li>
		<li>Communicating feedback from robots using affordances in addition to speech</li>
		<li>Spoken language generation for physically situated dialogue</li>
		
	<p>Researchers may choose to submit:</p>
	<ul>
	<li><b>Long papers and short papers</b> will present original research and go through the regular SIGdial peer review process by the general SIGdial program committee. These papers will appear in the main SIGdial proceedings and are presented with the main track. Long papers must be no longer than eight pages, including title, text, figures and tables, along with two additional pages for example discourses or dialogues and algorithms. Short papers should be no longer than four pages including title, text, figures and tables, along with one additional page for example discourses or dialogues and algorithms. An unlimited number of pages are allowed for references.</li>
	<li><b>Late-breaking and work-in-progress papers</b> will showcase ongoing work and focused, relevant contributions. Submissions need not present original work and are limited to four pages including references. These will be reviewed by the special session organizers and posted on the special session website. These papers will be presented as lightning talks or posters during the session. Authors will retain the copyright to their work so that they may submit to other venues as their work matures.</li>
	</ul>
		
	<h4>Long and short paper deadline: March 11 (<i>Final pdf due March 18</i>)</h4>
	<p>To submit a long or short paper, please go to the <a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL 2018 main page</a> for conference submissions (deadline March 11). When submitting, indicate "Physically Situated Dialogue" as the candidate special session. All long and short submissions must follow the SIGDIAL 2018 format.</p>

	<h4>Late-breaking and work-in-progress paper deadline: April 28</h4>
	<p>To submit a late-breaking or work-in-progress paper, please email a 2-4 page PDF (including references) formatted using the SIGDIAL 2018 format guidelines, to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a> by April 28. You will receive a confirmation of your submission and notification before the Early Bird Registration deadline.</p>
  
    <h3>Important Dates</h3>
    <table>
        <tbody><tr>
            <td>March 11, 2018</td>
            <td>Long and short paper initial submission deadline<br>
            (<a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL submission system</a>)</td>
        </tr>
		<tr>
			<td>March 18, 2018</td>
			<td>Final pdf due for long and short paper submission</td>
        <tr>
            <td>April 20, 2018</td>
            <td>SIGDIAL notification of acceptance</td>
        </tr>
        <tr>
            <td>April 28, 2018</td>
            <td>Late-breaking and work-in-progress submission deadline<br>
            (Email directly to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a>)</td>
        </tr>
            <td>May 13, 2018</td>
            <td>Camera-ready submission deadline</td>
        </tr>
        <tr>
            <td>July 12-14, 2018</td>
            <td>SIGDIAL conference</td>
        </tr>
    </tbody></table>

    <h3>Organizers</h3>

    <p><a href="http://www.seanandrist.com">Sean Andrist</a>, Microsoft Research<br>
    <a href="https://users.soe.ucsc.edu/~slukin">Stephanie Lukin</a>, Army Research Lab<br>
    <a href="http://www.cs.cmu.edu/~mrmarge">Matthew Marge</a>, Army Research Lab<br>
    <a href="https://jessethomason.com">Jesse Thomason</a>, University of Texas at Austin<br>
    <a href="http://www.cs.cmu.edu/~zhouyu">Zhou Yu</a>, University of California, Davis</p>
