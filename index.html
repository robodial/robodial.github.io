<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">

<title>Special Session on Physically Situated Dialogue</title>
<link rel="stylesheet" type="text/css" href="./files/style.css">
<meta name="Author" content="Sean Andrist">
<meta name="description" content="Special Session on Physically Situated Dialogue (SIGDIAL 2018)">
<meta name="keywords" content="dialogue, sigdial, robots, situated, conversation, artificial intelligence">
</head>

<body>

<div id="container">

<div id="sidebar">

    <ul>
        <li><a href="#overview">Overview</a></li>
        <li><a href="#call">Call for Papers</a></li>
		<li><a href="#submissions">Submissions</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <!--li><a href="#program">Program</a></li>
        <li><a href="#registration">Registration</a></li-->
        <li><a href="#organizers">Organizers</a></li>
    </ul>

</div>

<div class="section">
    <h2>Special Session on Physically Situated Dialogue (RoboDIAL)</h2>

    <h3>Part of <a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL 2018</a></h3>
    <h3>July 12-14, 2018<br></h3>

    RMIT University<br>
    Melbourne, Australia<br><br>
	
	<h4>Papers due March 11 (for SIGDIAL) or April 28 (for late-breaking and work-in-progress papers)</h4>

</div>

<div class="section"><a name="overview"></a>

    <h3>Overview</h3>

    <p>Recent technologies have brought conversational robots out of the lab and into the homes and workplaces of real users. Dialogue is now actively taking place with robots and other smart devices to understand, operate, navigate, and manipulate physical space. <i>Physically situated dialogue</i> distinguishes itself from other forms of dialogue in that it (1) takes place in a physical space, (2) refers to the shared surroundings of dialogue partners, and (3) involves a physical agent that can make actions in the world. There is a growing need for showcasing bi-directional dialogue work that draws on language grounding, models of vision and language, as well as dialogue that allows physically situated agents to ask for clarification and provide updates on their internal states.</p>
	
</div>

<div class="section"><a name="call"></a>

    <h3>Call for Papers</h3>

    <p>Our objectives in this special session are to showcase recent and ongoing work on physically situated dialogue, and to identify paths forward in this space from research across communities including dialogue, robotics, computer vision, NLP, and AI. The special session will feature presentations, a poster session, and a panel discussion comprising a mix of experts in the topic area. We welcome submissions on any topic related to physically situated dialogue, including but not limited to:</p>
    <ul>
		<li>Interaction studies with smart-home devices</li>
		<li>Learning from demonstration through natural language dialogue</li>
		<li>Explainable AI in physical spaces</li>
		<li>Representations of physical surroundings / world modeling to support grounded communication</li>
		<li>Embodied visual question answering and/or generation</li>
		<li>Empirical studies of human-robot dialogue (Wizard-of-Oz based, simulated, or semi-autonomous)</li>
		<li>Computational models of dialogue management and/or turn-taking with physical agents</li>
		<li>Methods of building or leveraging common ground with physical agents in real-world or simulated environments</li>
		<li>Corpora of physically situated dialogue (Wizard-of-Oz based or otherwise)</li>
		<li>Multimodal information processing to support dialogue (including speech, gaze, gesture)</li>
		<li>Physical embodiment, voice, or personification of robots and their effects on human-robot dialogue</li>
		<li>Communicating feedback from robots using affordances in addition to speech</li>
		<li>Spoken language generation for physically situated dialogue</li>
    </ul>
    <p></p>
</div>
	
<div class="section"><a name="submissions"></a>
		
	<p>Researchers may choose to submit:</p>
	<ul>
	<li><b>Long papers and short papers</b> will present original research and go through the regular SIGdial peer review process by the general SIGdial program committee. These papers will appear in the main SIGdial proceedings and are presented with the main track. Long papers must be no longer than eight pages, including title, text, figures and tables, along with two additional pages for example discourses or dialogues and algorithms. Short papers should be no longer than four pages including title, text, figures and tables, along with one additional page for example discourses or dialogues and algorithms. An unlimited number of pages are allowed for references.</li>
	<li><b>Late-breaking and work-in-progress papers</b> will showcase ongoing work and focused, relevant contributions. Submissions need not present original work and are limited to four pages including references. These will be reviewed by the special session organizers and posted on the special session website. These papers will be presented as lightning talks or posters during the session. Authors will retain the copyright to their work so that they may submit to other venues as their work matures.</li>
	</ul>
		
	<h4>Long and short paper deadline: March 11</h4>
	<p>To submit a long or short paper, please go to the <a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL 2018 main page</a> for conference submissions (deadline March 11). When submitting, indicate "Physically Situated Dialogue" as the candidate special session. All long and short submissions must follow the SIGDIAL 2018 format.</p>

	<h4>Late-breaking and work-in-progress paper deadline: April 28</h4>
	<p>To submit a late-breaking or work-in-progress paper, please email a 2-4 page PDF (including references) formatted using the SIGDIAL 2018 format guidelines, to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a> by April 28. You will receive a confirmation of your submission and notification before the Early Bird Registration deadline.</p>

</div>

<div class="section"><a name="dates"></a>
    <h3>Important Dates</h3>
    <table>
        <tbody><tr>
            <td>March 11, 2018</td>
            <td>Long and short paper submission deadline<br>
            (<a href="http://www.sigdial.org/workshops/conference19/">SIGDIAL submission system</a>)</td>
        </tr>
        <tr>
            <td>April 20, 2018</td>
            <td>SIGDIAL notification of acceptance</td>
        </tr>
        <tr>
            <td>April 28, 2018</td>
            <td>Late-breaking and work-in-progress submission deadline<br>
            (Email directly to: <a href="mailto:robodial@googlegroups.com">robodial@googlegroups.com</a>)</td>
        </tr>
            <td>May 13, 2018</td>
            <td>Camera-ready submission deadline</td>
        </tr>
        <tr>
            <td>July 12-14, 2018</td>
            <td>SIGDIAL conference</td>
        </tr>
    </tbody></table>
</div>

<!--div class="section"><a name="program"></a>

    <h3>Program</h3>

    <p>The symposium spans two-and-a-half days and will include invited and contributed talks, interactive sessions, and open discussions.</p>

    <p>Fifteen minutes are allocated for each contributed talk, followed by an additional five minutes for questions. Five minutes are allocated for each lightning talk, with each session followed by a joint, five minute question and answer session. Authors of lightning talk papers will then present their work during the subsequent poster session. We invite authors of contributed talk papers to participate in the poster session as well.</p>

    <p>If you would like to propose questions for the panel, please add them here: <a href="http://nchrc.ttic.edu/panel">http://nchrc.ttic.edu/panel</a>.</p>

    <h4 class="program">Thursday, November 9</h4>
    <table>
        <tbody><tr>
            <td class="time">09:00amâ€“09:10am</td>
            <td><span class="showcase">Welcoming Remarks</span></td>
        </tr>
        <tr>
            <td class="time">09:10amâ€“10:30am</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Ralph Hollis (Carnegie Mellon University)</span>, <span class="title">Physical Human-Robot Interaction with Dynamically Stable Mobile Robots</span></td>
        </tr>
        <tr>
            <td></td>
            <td><span class="showcase">Contributed Talks</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Junjie Hu, Desai Fan, Shuxin Yao, and Jean Oh</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/27.pdf">Natural Communication for Human-Robot Collaboration</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Adrian Boteanu, Jacob Arkin, Siddharth Patki, Thomas Howard, and Hadas Kress-Gazit</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/28.pdf">Robot-Initiated Specification Repair through Grounded Language Interaction</a></span></td>
        </tr>
        <tr>
            <td class="time">10:30amâ€“11:00am</td>
            <td><span class="showcase">Coffee Break</span></td>
        </tr>
        <tr>
            <td class="time">11:00amâ€“12:30pm</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Alborz Geramifard (Amazon)</span>, <span class="title">TBD</span></td>
        </tr>
        <tr>
            <td></td>
            <td><span class="showcase">Contributed Talks</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Divesh Lala, Koji Inoue, Pierrick Milhorat and Tatsuya Kawahara</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/6.pdf">Detection of Social Signals for Recognizing Engagement in Human-Robot Interaction</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Dan Bohus, Sean Andrist, and Eric Horvitz</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/17.pdf">A Study in Scene Shaping: Adjusting F-formations in the Wild</a></span></td>
        </tr>
        <tr>
            <td class="time">12:30pmâ€“02:00pm</td>
            <td><span class="showcase">Lunch</span></td>
        </tr>
        <tr>
            <td class="time">02:00pmâ€“03:30pm</td>
            <td><span class="showcase">Lightning Talks and Poster Session</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Casey R. Kennington and Sarah Plane</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/1.pdf">Symbol, Conversational, and Societal Grounding with a Toy Robot</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Peggy Wu</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/7.pdf">Human Physical Movements for Kinematic Learning for Robots</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Douglas Summers-Stay and Dandan Li</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/8.pdf">Analogical Reasoning with Knowledge-based Embeddings</a></span></td>
        </tr>

        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Mara Brandt, Britta Wrede, Franz Kummert, and Lars Schillingmann</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/10.pdf">Confirmation Detection in Human-Agent Interaction Using Non-Lexical Speech Cues</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Raj M Korpan, Susan Epstein, Anoop Aroor, and Gil Dekel</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/11.pdf">WHY: Natural Explanations from a Robot Navigator</a></span></td>
        </tr>
        <tr>
            <td class="time">03:30pmâ€“04:00pm</td>
            <td><span class="showcase">Coffee Break</span></td>
        </tr>
        <tr>
            <td class="time">04:00pmâ€“04:45pm</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Rohan Paul (Massachusett Institute of Technology)</span>, <span class="title">Leveraging Visual-Linguistic Context for Grounding Natural Language Instructions</span></td>
        </tr>
        <tr>
            <td class="time">04:45pmâ€“05:30pm</td>
            <td><span class="showcase">Panel Discussion</span><span class="author"> (Suggest questions <a href="http://nchrc.ttic.edu/panel">here</a>)</span></td>
        </tr>
    </tbody></table>


    <h4 class="program">Friday, November 10</h4>
    <table>
        <tbody><tr>
            <td class="time">09:00amâ€“09:45am</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Ben Kuipers (University of Michigan)</span>, <span class="title">TBD</span></td>
        </tr>
        <tr>
            <td class="time">09:45amâ€“10:30am</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Joyce Chai (Michigan State University)</span>, <span class="title">TBD</span></td>
        </tr>
        <tr>
            <td class="time">10:30amâ€“11:00am</td>
            <td><span class="showcase">Coffee Break</span></td>
        </tr>
        <tr>
            <td class="time">11:00amâ€“11:45am</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Yejin Choi (University of Washington)</span>, <span class="title">TBD</span></td>
        </tr>
        <tr>
            <td class="time">11:45amâ€“12:30pm</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Dan Bohus (Microsoft Research)</span>, <span class="title">Engagement and Turn-Taking in Physically Situated Language Interaction</span></td>
        </tr>
        <tr>
            <td class="time">12:30pmâ€“02:00pm</td>
            <td><span class="showcase">Lunch</span></td>
        </tr>
        <tr>
            <td class="time">02:00pmâ€“03:30pm</td>
            <td><span class="showcase">Lightning Talks and Poster Session</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Michael Wollowski, Carlotta Berry, Ryder Winck, Alan Jern, David Voltmer, Alan Chiu, and Yosi Shibberu</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/13.pdf">A Data-driven Approach Towards Human-robot Collaborative Problem Solving in a Shared Space</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Qiaozi Gao, Lanbo She, and Joyce Chai</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/15.pdf">Interactive Learning of State Representations through Natural Language Instruction and Explanation</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Stephanie Zhou, Alane Suhr, and Yoav Artzi</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/18.pdf">Visual Reasoning with Natural Language</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Dipendra Misra and Yoav Artzi</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/19.pdf">Reinforcement Learning for Mapping Instructions to Actions with Reward Learning</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Szrung Shiang, Jean Oh, and Anatole Gershman</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/20.pdf">A Generalized Model for Multimodal Perception</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Dianna Radpour and Vinay Ashokkumar</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/26.pdf">Non-Contextual Sarcasm Modeling with Neural Network Benchmarking</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Nicole K Glabinski, Rohan Paul, and Nicholas Roy</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/30.pdf">Grounding Natural Language Instructions with Unknown Object References using Learned Visual Attributes</a></span></td>
        </tr>
        <tr>
            <td class="time">03:30pmâ€“04:00pm</td>
            <td><span class="showcase">Coffee Break</span></td>
        </tr>
        <tr>
            <td class="time">04:00pmâ€“05:00pm</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">David Traum (University of Southern California)</span>, <span class="title">TBD</span></td>
        </tr>
        <tr>
            <td></td>
            <td><span class="showcase">Contributed Talk</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Megan Zimmerman and Jeremy Marvel</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/4.pdf">Smart Manufacturing and The Promotion of Artificially-Intelligent Human-Robot Collaborations in Small- and Medium-sized Enterprises</a></span></td>
        </tr>
        <tr>
            <td class="time">05:00pmâ€“05:30pm</td>
            <td><span class="showcase">Panel Discussion</span><span class="author"> (Suggest questions <a href="http://nchrc.ttic.edu/panel">here</a>)
        </span></td></tr>
    </tbody></table>


    <h4 class="program">Saturday, November 11</h4>
    <table>
        <tbody><tr>
            <td class="time">09:00amâ€“10:30am</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Ray Mooney (University of Texas, Austin)</span>, <span class="title">Robots that Learn Grounded Language Through Interactive Dialog</span></td>
        </tr>
        <tr>
            <td></td>
            <td><span class="showcase">Contributed Talks</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Sergei Nirenburg and  Peter Wood</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/3.pdf">Toward Human-Style Learning in Robots</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Andrea F. Daniele, Thomas Howard, and Matthew R. Walter</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/29.pdf">Learning Articulated Object Models from Language and Vision</a></span></td>
        </tr>
        <tr>
            <td class="time">10:30amâ€“11:00am</td>
            <td><span class="showcase">Coffee Break</span></td>
        </tr>
        <tr>
            <td class="time">11:00amâ€“12:30pm</td>
            <td><span class="showcase">Invited Talk</span></td>
        </tr>
        <tr>
            <td></td>
            <td class="details"><span class="author">Alex Rudnicky (Carnegie Mellon University)</span>, <span class="title">Blended Conversations</span></td>
        </tr>
        <tr>
            <td></td>
            <td><span class="showcase">Contributed Talks</span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Claire N Bonial, Matthew Marge, Ron Artstein, Felix Gervits, Cory Hayes, Cassidy Henry, Susan Hill, Anton Leuski, Pooja Moolchandani, Kimberly Pollard, David Traum, Clare Voss, Ashley Foots, and Stephanie M Lukin</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/12.pdf">Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue</a></span></td>
        </tr>
        <tr>
            <td class="time"></td>
            <td class="details"><span class="author">Nakul Gopalan, Edward Williams, Stefanie Tellex, and Mina Rhee</span>, <span class="title"><a href="http://www.ttic.edu/nchrc/papers/22.pdf">Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision</a></span></td>
        </tr>







        <tr>
            <td class="time">12:30pmâ€“12:45pm</td>
            <td><span class="showcase">Closing Remarks</span></td>
        </tr>
    </tbody></table>


</div-->

<!--div class="section"><a name="registration"></a>

    <h3>Registration</h3>

    <p>Please register for the symposium through the <a href="https://www.aaai.org/Symposia/Fall/fss17.php">AAAI 2017 Fall Symposium site</a>. Note that the deadline for registration is October 13, 2017.</p>


</div-->



<div class="section"><a name="organizers"></a>

    <h3>Organizers</h3>

    <p><a href="http://www.seanandrist.com">Sean Andrist</a>, Microsoft Research<br>
    <a href="https://users.soe.ucsc.edu/~slukin">Stephanie Lukin</a>, Army Research Lab<br>
    <a href="http://www.cs.cmu.edu/~mrmarge">Matthew Marge</a>, Army Research Lab<br>
    <a href="https://jessethomason.com">Jesse Thomason</a>, University of Texas at Austin<br>
    <a href="http://www.cs.cmu.edu/~zhouyu">Zhou Yu</a>, University of California, Davis</p>

</div>

	</div>
</body>
</html>